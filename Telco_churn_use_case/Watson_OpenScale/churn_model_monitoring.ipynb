{"cells": [{"metadata": {}, "cell_type": "markdown", "source": "# <img src=\"https://github.com/pmservice/ai-openscale-tutorials/raw/master/notebooks/images/banner.png\" align=\"left\" alt=\"banner\">"}, {"metadata": {"collapsed": true}, "cell_type": "markdown", "source": "# Monitor your ML Models using Watson OpenScale and WML on Cloud Pak for Data"}, {"metadata": {}, "cell_type": "markdown", "source": "## 1. Setup the Notebook Environment"}, {"metadata": {}, "cell_type": "markdown", "source": "## 1.1 Install the necessary packages"}, {"metadata": {}, "cell_type": "markdown", "source": "### Watson OpenScale Python SDK"}, {"metadata": {}, "cell_type": "code", "source": "!pip install ibm-ai-openscale", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Scikit-learn version 0.22\n"}, {"metadata": {}, "cell_type": "code", "source": "!pip install scikit-learn==0.22.0", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Watson Machine Learning Python SDK"}, {"metadata": {}, "cell_type": "code", "source": "!pip install --upgrade watson-machine-learning-client-V4==1.0.93 | tail -n 1", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Restart the Notebook after Installing the required packages. By clicking on `Kernel>Restart`"}, {"metadata": {}, "cell_type": "markdown", "source": "## 1.2 Import Packages"}, {"metadata": {}, "cell_type": "code", "source": "from sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.pipeline import FeatureUnion\nfrom sklearn import preprocessing\nfrom sklearn import svm, metrics\nfrom scipy import sparse\nfrom watson_machine_learning_client import WatsonMachineLearningAPIClient\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\nimport json\nimport ibm_db\n\n\nimport pandas as pd\nimport numpy as np\n\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split, GridSearchCV\n\nfrom ibm_ai_openscale import APIClient4ICP\nfrom ibm_ai_openscale.engines import *\nfrom ibm_ai_openscale.utils import *\nfrom ibm_ai_openscale.supporting_classes import PayloadRecord, Feature\nfrom ibm_ai_openscale.supporting_classes.enums import *", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## 2. Configuration"}, {"metadata": {}, "cell_type": "markdown", "source": "### 2.1 Global Variables"}, {"metadata": {}, "cell_type": "code", "source": "MODEL_NAME=\"telco_churn_monitored_srs\"\nDEPLOYMENT_NAME=\"telco_churn_monitored_srs_deployment\"\n# Ensure you create a an empty Schema and store the name in this variable\n# SCHEMA_NAME=\"JLQ22844\"\n\n# Enter the Deployment Space you have associated project with \ndep_name=\"telco_churn_deployment_space_srs\"", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 2.2 Add Dataset\n\nSelect the `Insert Pandas Dataframe` option, after selecting the below cell. Ensure the variable name is `df_data_1`"}, {"metadata": {}, "cell_type": "code", "source": "# Place cursor below and insert the Pandas DataFrame for the Telco churn data\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 2.3 Update your AIOS Credentials"}, {"metadata": {}, "cell_type": "code", "source": "WOS_CREDENTIALS={\n    \"url\" : os.environ['RUNTIME_ENV_APSX_URL'],\n    \"username\":\"XXXXXXX\",\n    \"password\":\"XXXXXXX\"\n}", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 2.4 Input your WML Credentials \n"}, {"metadata": {}, "cell_type": "code", "source": "import sys,os,os.path\n\n\n# WML_CREDENTIALS = {\n# \"token\": os.environ['USER_ACCESS_TOKEN'],\n# \"instance_id\" : \"wml_local\",\n# \"url\" : os.environ['RUNTIME_ENV_APSX_URL'],\n# \"version\": \"3.0.0\"\n# }\nWML_CREDENTIALS = WOS_CREDENTIALS.copy()\nWML_CREDENTIALS['instance_id']='openshift'\nWML_CREDENTIALS['version']='3.0.0'", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 2.5 Add your Db credentials\n\n#### These Db credentials are needed ONLY if you have NOT configured your `OpenScale Datamart`."}, {"metadata": {}, "cell_type": "code", "source": "\n# DATABASE_CREDENTIALS = {\n#     \"hostname\": \"dashdb-txn-sbox-yp-dal09-11.services.dal.bluemix.net\",\n#     \"username\": \"tzm22305\",\n#     \"password\": \"s2knhr3znx-c5s03\",\n#     \"port\": 50000,\n#     \"db\": \"BLUDB\",\n    \n# }\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## 3. Create the Customer Churn Model using Scikit-Learn"}, {"metadata": {}, "cell_type": "code", "source": "X=df_data_1.drop(['Churn'], axis=1)\ny=df_data_1.loc[:, 'Churn']", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "X.head()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "'''Add a categorical transformer to your model pipeline. \n    You will need to add a label encoder into the model pipeline before storing it into WML '''\n\ncategorical_features = [\"gender\", \"SeniorCitizen\", \"Partner\", \"Dependents\", \"PhoneService\", \"MultipleLines\", \"InternetService\", \"OnlineSecurity\",\n                        \"OnlineBackup\", \"DeviceProtection\", \"TechSupport\", \"StreamingTV\", \"StreamingMovies\", \"Contract\", \"PaperlessBilling\", \"PaymentMethod\"]\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "preprocessor = ColumnTransformer(\n    transformers=[\n        ('cat', categorical_transformer, categorical_features)])", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n                      ('model', svm.SVC(kernel='linear'))])", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "model = pipeline.fit(X_train,y_train)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "from sklearn.metrics  import accuracy_score, classification_report\ny_scores = model.predict(X_test)\nprint(accuracy_score(y_test, y_scores))\nprint(classification_report(y_test, y_scores))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## 4. Create a new Deployment"}, {"metadata": {}, "cell_type": "code", "source": "client = WatsonMachineLearningAPIClient(WML_CREDENTIALS)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "meta_props={\n client.repository.ModelMetaNames.NAME: MODEL_NAME,\n client.repository.ModelMetaNames.RUNTIME_UID: \"scikit-learn_0.22-py3.6\",\n client.repository.ModelMetaNames.TYPE: \"scikit-learn_0.22\",\n}", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "\nproject_id = os.environ['PROJECT_ID']\nclient.set.default_project(project_id)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "def guid_from_space_name(client, space_name):\n\n    instance_details = client.service_instance.get_details()\n\n    space = client.spaces.get_details()\n    res=[]\n    for item in space['resources']: \n        if item['entity'][\"name\"] == space_name:\n            res=item['metadata']['guid']\n\n    return res", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Enter the name of your deployment space of the current project\n\nspace_uid = guid_from_space_name(client, dep_name)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "space_uid", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "client.set.default_space(space_uid)\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Store, Deploy and Score your Custom WML Model"}, {"metadata": {}, "cell_type": "code", "source": "deploy_meta = {\n     client.deployments.ConfigurationMetaNames.NAME: DEPLOYMENT_NAME,\n     client.deployments.ConfigurationMetaNames.ONLINE: {}\n }", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "## Store the model on WML\npublished_model = client.repository.store_model(pipeline,\n                                             meta_props=meta_props,\n                                             training_data=X_train,\n                                             training_target=y_train\n                                                )\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "published_model_uid = client.repository.get_model_uid(published_model)", "execution_count": null, "outputs": []}, {"metadata": {"scrolled": true}, "cell_type": "code", "source": "## Create a Deployment for your stored model\n\ncreated_deployment = client.deployments.create(published_model_uid, meta_props=deploy_meta)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "\nscoring_endpoint = None\ndeployment_uid=created_deployment['metadata']['guid']", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## 5. Setup your Watson Openscale Dashboard "}, {"metadata": {}, "cell_type": "markdown", "source": "### 5.1 Create the Watson Openscale Client"}, {"metadata": {}, "cell_type": "code", "source": "ai_client = APIClient4ICP(aios_credentials=WOS_CREDENTIALS)\nai_client.version", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 5.2 Setup the Datamart on AI OpenScale"}, {"metadata": {}, "cell_type": "code", "source": "try:\n    data_mart_details = ai_client.data_mart.get_details()\n    print('Using existing external datamart')\nexcept:\n    print('Setting up external datamart')\n    ai_client.data_mart.setup(db_credentials=DATABASE_CREDENTIALS, schema=SCHEMA_NAME)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "data_mart_details = ai_client.data_mart.get_details()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "data_mart_details", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 5.3 Add your Machine Learning Provider\n\nIf you have already bound the ML Provider to the Openscale instance, then just retrieve the binding_uid, by commenting first line and uncommenting the second line"}, {"metadata": {}, "cell_type": "code", "source": "WML_CREDENTIALS", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "binding_uid = ai_client.data_mart.bindings.add('WML instance - churn_srs', WatsonMachineLearningInstance4ICP(wml_credentials=WML_CREDENTIALS))\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "ai_client.data_mart.bindings.list_assets()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 5.4 Perform Initial Scoring for your Model Deployment\n"}, {"metadata": {}, "cell_type": "code", "source": "score=X_test.tail(20)\nscore", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "scoring_data=list(list(x) for x in zip(*(score[x].values.tolist() for x in score.columns)))\nscoring_data", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "fields=list(X_test.columns)\nprint(len(fields))\nfields, scoring_data[0]", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "\njob_payload = {\nclient.deployments.ScoringMetaNames.INPUT_DATA: [{\n 'values': scoring_data\n}]\n}\nprint(job_payload)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "scoring_response = client.deployments.score(deployment_uid, job_payload)\n\nprint(scoring_response)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 5.5 Create a new Subscription "}, {"metadata": {}, "cell_type": "code", "source": "subscription = ai_client.data_mart.subscriptions.add(WatsonMachineLearningAsset(\n    published_model_uid,\n    problem_type=ProblemType.BINARY_CLASSIFICATION,\n    input_data_type=InputDataType.STRUCTURED,\n    label_column='Churn',\n    prediction_column='prediction',\n    probability_column='prediction_probability',\n    categorical_columns=[\"gender\", \"SeniorCitizen\", \"Partner\", \"Dependents\", \"PhoneService\", \"MultipleLines\", \"InternetService\", \"OnlineSecurity\",\n                        \"OnlineBackup\", \"DeviceProtection\", \"TechSupport\", \"StreamingTV\", \"StreamingMovies\", \"Contract\", \"PaperlessBilling\", \"PaymentMethod\"],\n    feature_columns = [\"gender\", \"SeniorCitizen\", \"Partner\", \"Dependents\", \"PhoneService\", \"MultipleLines\", \"InternetService\", \"OnlineSecurity\",\n                        \"OnlineBackup\", \"DeviceProtection\", \"TechSupport\", \"StreamingTV\", \"StreamingMovies\", \"Contract\", \"PaperlessBilling\", \"PaymentMethod\", \"tenure\",\"TotalCharges\",\"MonthlyCharges\"],\n))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "subscriptions_uids = ai_client.data_mart.subscriptions.get_uids()\nai_client.data_mart.subscriptions.list()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 5.6 Perform Inital Payload Logging\nNote: You may re-use this code snippet by modifying the request_data variable to perform payload logging after finishing the initial dashboard setup"}, {"metadata": {}, "cell_type": "code", "source": "fields=list(X_test.columns)\n\nrequest_data = {\n    \"fields\": fields,\n    \"values\": scoring_data\n  }\nrequest_data", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "**<font color='red'><< REPLACE subscription_uid BELOW with the uid for your subscription. For e.g.<br/>subscription_uid=\"644e4e6d-8a82-4f07-9489-381d44469a23\" >></font>**"}, {"metadata": {}, "cell_type": "code", "source": "## From the output of the above table choose your model name and copy the uid against it. Store the uid in the subscription_uid variable\n\n\nsubscription_uid=\"5664ad39-2363-44bd-bbdc-fbce71bbb575\"\nfrom ibm_ai_openscale import APIClient4ICP\nfrom ibm_ai_openscale.supporting_classes import PayloadRecord\n\n\nsubscription = ai_client.data_mart.subscriptions.get(subscription_uid=subscription_uid)\n\n\nrecords = [PayloadRecord(request=request_data, response=scoring_response, response_time=18), \n                PayloadRecord(request=request_data, response=scoring_response, response_time=12)]\n\nsubscription.payload_logging.store(records=records)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 5.7 Setup Quality Monitoring\n\n```NOTE: If you are using the dataset provided in the dashboard, leave the threshold monitors to these values. However, if you are using your own dataset, you can play around with the threshold value (value b/w 0 and 1) according to your requirement.```"}, {"metadata": {}, "cell_type": "code", "source": "time.sleep(5)\nsubscription.quality_monitoring.enable(threshold=0.90, min_records=5)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 5.8 Log Feedback Data to your Subscription"}, {"metadata": {}, "cell_type": "code", "source": "feedback_data_raw=pd.concat([X_test,y_test],axis=1)\nfeedback_data_raw", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "feedback_data=feedback_data_raw.tail(20).values.tolist()\nfeedback_data", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "feedback_scoring={\n    \"data\":feedback_data\n}", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "subscription.feedback_logging.store(feedback_scoring['data'])\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "subscription.feedback_logging.show_table()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "#### Run an inital quality test"}, {"metadata": {}, "cell_type": "code", "source": "run_details = subscription.quality_monitoring.run(background_mode=False)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "subscription.quality_monitoring.show_table()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "%matplotlib inline\n\nquality_pd = subscription.quality_monitoring.get_table_content(format='pandas')\nquality_pd.plot.barh(x='id', y='value');", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 5.9 Setup the Fairness Monitors\n\nThe code below configures fairness monitoring for our model. It turns on monitoring for two features, _conds(Weather Condition) and Traffic for the cell tower. In each case, we must specify:\n  * Which model feature to monitor\n  * One or more **majority** groups, which are values of that feature that we expect to receive a higher percentage of favorable outcomes\n  * One or more **minority** groups, which are values of that feature that we expect to receive a higher percentage of unfavorable outcomes\n  * The threshold at which we would like OpenScale to display an alert if the fairness measurement falls below (in this case, 95%)\n\nAdditionally, we must specify which outcomes from the model are favourable outcomes, and which are unfavourable. We must also provide the number of records OpenScale will use to calculate the fairness score. In this case, OpenScale's fairness monitor will run hourly, but will not calculate a new fairness rating until at least 5 records have been added. Finally, to calculate fairness, OpenScale must perform some calculations on the training data, so we provide the dataframe containing the data."}, {"metadata": {}, "cell_type": "code", "source": "subscription.fairness_monitoring.enable(\n            features=[\n                Feature(\"gender\", majority=['Male'], minority=['Female'], threshold=0.95),\n            ],\n            favourable_classes=[\"No\"],\n            unfavourable_classes=[\"Yes\"],\n            min_records=50,\n            training_data=df_data_1\n        )", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "score2=X_test.head(50)\n\nscoring_data2=list(list(x) for x in zip(*(score2[x].values.tolist() for x in score2.columns)))\n\nfields2=list(X_test.columns)\n\njob_payload2 = {\nclient.deployments.ScoringMetaNames.INPUT_DATA: [{\n 'values': scoring_data2\n}]\n}\n\nscoring_response2 = client.deployments.score(deployment_uid, job_payload2)\n\n\nrequest_data2 = {\n    \"fields\": fields,\n    \"values\": scoring_data2\n  }\n\nrecords2 = [PayloadRecord(request=request_data2, response=scoring_response2, response_time=18), \n                PayloadRecord(request=request_data2, response=scoring_response2, response_time=12)]\n\nsubscription.payload_logging.store(records=records2)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "time.sleep(20)\n\nrun_details = subscription.fairness_monitoring.run(background_mode=False)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "time.sleep(5)\n\nsubscription.fairness_monitoring.show_table()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## 6.0 Custom monitors and metrics\n"}, {"metadata": {}, "cell_type": "markdown", "source": "### 6.1 Register custom monitor"}, {"metadata": {}, "cell_type": "code", "source": "def get_definition(monitor_name):\n    monitors_definitions = ai_client.data_mart.monitors.get_details()['monitor_definitions']\n    \n    for definition in monitors_definitions:\n        if monitor_name == definition['entity']['name']:\n            return definition\n    \n    return None", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Change `monitor_name` to something unique."}, {"metadata": {}, "cell_type": "code", "source": "from ibm_ai_openscale.supporting_classes import Metric, Tag\n\nmonitor_name = 'custom_monitor_shivam'\nmetrics = [Metric(name='sensitivity', lower_limit_default=0.8), Metric(name='specificity', lower_limit_default=0.75)]\ntags = [Tag(name='region', description='customer geographical region')]\n\nexisting_definition = get_definition(monitor_name)\n\nif existing_definition is None:\n    my_monitor = ai_client.data_mart.monitors.add(name=monitor_name, metrics=metrics, tags=tags)\nelse:\n    my_monitor = existing_definition", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 6.1.1 Get monitors uids and details"}, {"metadata": {}, "cell_type": "code", "source": "monitor_uid = my_monitor['metadata']['guid']\n\nprint(monitor_uid)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "my_monitor = ai_client.data_mart.monitors.get_details(monitor_uid=monitor_uid)\nprint('monitor definition details', my_monitor)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## 6.2 Enable custom monitor for subscription"}, {"metadata": {}, "cell_type": "code", "source": "from ibm_ai_openscale.supporting_classes import Threshold\n\nthresholds = [Threshold(metric_uid='sensitivity', lower_limit=0.9)]\nsubscription.monitoring.enable(monitor_uid=monitor_uid, thresholds=thresholds)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 6.2.1 Get monitor configuration details"}, {"metadata": {}, "cell_type": "code", "source": "subscription.monitoring.get_details(monitor_uid=monitor_uid)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## 6.3 Storing custom metrics"}, {"metadata": {}, "cell_type": "code", "source": "metrics = {\"specificity\": 0.78, \"sensitivity\": 0.67, \"region\": \"us-south\"}\n\nsubscription.monitoring.store_metrics(monitor_uid=monitor_uid, metrics=metrics)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 6.3.1 List and get custom metrics"}, {"metadata": {}, "cell_type": "code", "source": "subscription.monitoring.show_table(monitor_uid=monitor_uid)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "custom_metrics = subscription.monitoring.get_metrics(monitor_uid=monitor_uid, deployment_uid='credit')\ncustom_metrics", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "custom_metrics_pandas = subscription.monitoring.get_table_content(monitor_uid=monitor_uid)\n\n%matplotlib inline\ncustom_metrics_pandas.plot.barh(x='id', y='value');", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## 7.0 Payload analytics\n"}, {"metadata": {}, "cell_type": "markdown", "source": "### 7.1 Run data distributions calculation"}, {"metadata": {}, "cell_type": "code", "source": "from datetime import datetime\n\nstart_date = \"2018-01-01T00:00:00.00Z\"\nend_date = datetime.utcnow().isoformat() + \"Z\"\n\nsex_distribution = subscription.payload_logging.data_distribution.run(\n            start_date=start_date,\n            end_date=end_date,\n            group=['prediction', 'gender'],\n            agg=['count'])", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 7.2 Get data distributions as pandas dataframe\n"}, {"metadata": {}, "cell_type": "code", "source": "sex_distribution_run_uid = sex_distribution['id']\ndistributions_pd = subscription.payload_logging.data_distribution.get_run_result(run_id=sex_distribution_run_uid, format='pandas')\ndistributions_pd", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "subscription.payload_logging.data_distribution.show_chart(sex_distribution_run_uid);", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## 8. Identify transactions for Explainability"}, {"metadata": {}, "cell_type": "code", "source": "from ibm_ai_openscale.supporting_classes import *\nsubscription.explainability.enable(training_data=df_data_1)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "payload_data = subscription.payload_logging.get_table_content(limit=60)\npayload_data.filter(items=['scoring_id', 'predictedLabel', 'probability'])", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Add some more Payload (Optional for populating your dashboard)\n\nIf you wish to add some Payload Data. Take different sections of your test dataset and send to OpenScale as shown below-"}, {"metadata": {}, "cell_type": "code", "source": "score100=X_test.head(100)\nscore100", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "scoring_data100=list(list(x) for x in zip(*(score100[x].values.tolist() for x in score100.columns)))\nscoring_data100", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "fields=list(X_test.columns)\nprint(len(fields))\nfields, scoring_data100[0]", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "request_data100 = {\n    \"fields\": fields,\n    \"values\": scoring_data100\n  }\nrequest_data100", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "records100 = [PayloadRecord(request=request_data100, response=scoring_response, response_time=18), \n                PayloadRecord(request=request_data100, response=scoring_response, response_time=12)]\n\nsubscription.payload_logging.store(records=records100)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.6", "language": "python"}, "language_info": {"name": "python", "version": "3.6.10", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 1}